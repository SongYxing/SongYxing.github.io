<!-- <script src="https://www.google.com/jsapi" type="text/javascript"></script> -->
<!-- <script type="text/javascript">google.load("jquery", "1.3.2");</script> -->

<html>
<head>
    <title>NegVSR</title>
    <meta property="og:image" content="resources/teaser.jpeg"/>
    <meta property="og:title" content="Zero-shot Image-to-Image Translation"/>
    <meta property="og:description" content="Zero-shot Image-to-Image Translation"/>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.1/jquery.min.js"></script>
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.14.0/css/all.css">
    <link rel="stylesheet" href="styles/main.css">
    <!-- <link rel="stylesheet" href="styles/image_card_flip.css"> -->
    <link rel="stylesheet" href="styles/image_card_fader.css">
    <link rel="stylesheet" href="styles/image_card_slider.css">
</head>

<style>
    video{
        width: 100%;
        height: 100%;
    }
</style>

<body>
<br>

    <center>
    <span style="font-size:36px">NegVSR:Augmenting Negatives for Generalized Noise Modeling in Real-world Video Super-Resolution</span>
    <table align=center width=600px>
        
        <br>
        <table align=center width=250px>
            <tr>
                <td align=center width=120px>
                    <center>
                        <!-- <span style="font-size:24px"><a href='TODO'>[Paper]</a></span> -->
                        <span style="font-size:24px; margin-left: 0px;"><a href=''>[Code]</a></span>
                    </center>
                </td>
            </tr>
        </table>
    </table>
    </center>
    
    <hr>
    <center><h1>Abstract</h1></center>
    <p> 
        The capability of video super-resolution (VSR) to synthesize high-resolution video from ideal datasets has been demonstrated in many works. 
        However, applying the VSR model to real-world videos with unknown and complex degradation remains a challenging task. 
        First, existing degradation metrics in most VSR methods are not able to simulate the real-world noise and blur effectively. 
        On the contrary, a simple combinations of classical degradation were used for real-world noise modeling, 
        which lead to VSR model often be violated by out-of-distribution noise. Second, many SR models focus on noise simulation and transfer, 
        nevertheless, the sampled noise is monotonous and limited. To address the aforementioned problems, 
        we propose a Negatives augmentation strategy for generalized noise modeling in Video Super-Resolution (NegVSR) task. 
        Specifically, we first propose sequential noise generation toward real-world data to extract practical noise sequences. 
        Then, the degeneration domain is widely expanded by negative augmentation to build up a various yet challenging real-world noise set. 
        We further propose the augmented negative guidance loss to effectively learn robust features among augmented negatives. 
        Extensive experiments on real-world datasets (e.g., VideoLQ and FLIR) show that our method outperforms state-of-the-art methods with clear margin, especially in visual quality.
        <br>
        <span style="font-weight: 800;">Keywords:</span> real-world video super-resolution, noise sequences sampling, negative augmentation, augmented negative guidance. 
    
    </p>
    <hr>

    <br>
    <center><h1>Framework</h1></center>
    <center>
        <table align=center width=1000px>
            <tr>
                <td width=260px>
                    <center>
                        <img class="round" src="./assets/images/framework.jpg" style="width:900px"/>
                    </center>
                </td>
            </tr>
        </table>
    </center>
    <p> 
        The overview of the proposed NegVSR. Given an video V<sub>hr</sub>, an OOD video noise dataset V<sub>od</sub> and a low-resolution video V<sub>lr</sub>, 
        which is degenerated from  V<sub>hr</sub>. And the I<sub>t</sub> refers to a frame of  V<sub>hr</sub>. 
        (a) Our approach initially extracts noise-prone regions sequences through window sequences C movement in an unsupervised noise sample. 
        Subsequently, it mixes noise sequence N<sub>sq</sub> and V<sub>lr</sub> to create novel training input V<sub>lr</sub><sup>N</sup>. 
        (b) The mixed sequence V<sub>lr</sub><sup>N</sup> be applied with a patch-based auto-giration to derive V<sub>neg</sub>.  
        (c) Both  V<sub>neg</sub> and V<sub>lr</sub> are fed into VSR model to generate <span style="font-family: 'Times New Roman', Times, serif; font-size: 18px; position: relative; top: 0px;">&#x0059;&#x0302;</span> and Y, respectively. 
        In this component, L<sub>Aug-P</sub> enables the model to recover more effective pixels from the V<sub>lr</sub>. 
    </p> 
    


    <hr>
    <br>
    <h1>Method</h1>
    <center>
        <table align=center width=1000px style="margin-bottom: 20px;">
            <tr>
                <td width=1000px>
                    <center>
                        <img class="round" src="./assets/images/loss.jpg" style="width:1000px"/>
                    </center>
                </td>
            </tr>
        </table>
    </center>
    <p> 
        The figure depicts the process of our Augmented Negative Guidance approach. 
        First we obtain the positive output <span style="font-family: 'Times New Roman', Times, serif; font-size: 18px; position: relative; top: 0px;">&#x0059;&#x0302;</span> 
        by passing V<sub>hr</sub> through the degeneration model D and VSR. Then we inject noise sequence N<sub>sq</sub> into the degraded video during the Negative Pairs stage, 
         and feeding it to the VSR after being destroyed by NegMix. We perform Negative Pairs and Positive Pairs consecutively, 
         and share the weight between them. Finally we encourage Y and <span style="font-family: 'Times New Roman', Times, serif; font-size: 18px; position: relative; top: 0px;">&#x0059;&#x0302;</span> get closer to each other.
    </p>
    <br>
    

    <center>
        <h2><span style="font-weight: bold;">Results: </span>compare with other method</h2>
        <table align=center width=1100px>
            <tr>
                <td width=260px>
                    <center>
                        <img class="round" src="./assets/images/comparison.jpg" style="width:1100px"/>
                    </center>
                </td>
            </tr>
        </table>

        <center><h2>More details:</h2></center>
        <table align=center width=1100px>
            <tr>
                <td width=260px>
                    <center>
                        <img class="round" src="./assets/images/appendix_comparsion.jpg" style="width:1100px"/>
                    </center>
                </td>
            </tr>
        </table>


        <hr>
        <h2><span style="font-weight: bold;">Demos
        <table align=center width=1100px>
            <tr>
                <td width=260px>
                    <center>
                        <video controls>
                            <source src="assets/videos/023.mp4" type="video/mp4" style="width:1100px">
                        </video>
                    </center>
                </td>
            </tr>
        </table>
      
        <table align=center width=1100px>
            <tr>
                <td width=260px>
                    <center>
                        <video controls>
                            <source src="assets/videos/028.mp4" type="video/mp4" style="width:1100px">
                        </video>
                    </center>
                </td>
            </tr>
        </table>

    </center>
    
    
    


    <hr>

    <hr>
    <script>
        // $(".flip-card").click(function() {
        //     console.log("flipping")
        //   $(this).toggleClass("flipped");
        // });

        // $(".flip-card").mouseenter(function() {
        //     console.log("flipping")
        //   $(this).toggleClass("flipped");
        // });

        // $(".flip-card").mouseleave(function() {
        //     console.log("flipping")
        //   $(this).toggleClass("flipped");
        // });

        $(".flip-card").mouseenter(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("out");
            $(div_front).removeClass("in");

            $(div_back).addClass("in");
            $(div_back).removeClass("out");

        });

        $(".flip-card").mouseleave(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("in");
            $(div_front).removeClass("out");

            $(div_back).addClass("out");
            $(div_back).removeClass("in");

        });



        // $(".card-front").click(function() {
        //     card_back = $(this).next()[0];
        //     $(card_back).animate({
        //         width: "toggle",
        //     }, "slow");
            
        // });
    </script>
    <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    
</body>
</html>

